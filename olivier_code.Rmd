---
title: "Memoire R"
output: html_notebook
---

```{r}
library(tidyverse)
library(readr)
library(rtweet)
library(lubridate)
library(dplyr)
library(data.table)
library(cleanNLP)
library(foreach)
```

```{r}
#setwd("C:/Users/Olivier/Desktop/Cours_IREN/Memoire/")
#data_covid50 <- readRDS("sample_50k.rds")
#data_covid <- readRDS("df_0001002.rds")
```

```{r fichier de travail}
#takes of sample of subset of n records so it's easier to work with
#sample <- sample_n(data_covid, 50000)
#sample <- mutate(sample, )
#head1kk <-head(data_covid, 1000)
#samplen30k <- sample_n(data_covid, 30000)
#saveRDS(head1kk,"head1k.rds")
test <- readRDS("head1k.rds")
```

```{r compter le nombre de mots}
test$nb_mots<-ifelse(is.na(test$text),0,str_count(test$text, " ")+1) # l'astuce : compter les espaces et ajouter 1, pour compter les mots, on teste aussi les valeurs N/A pour ne pas avoir une valeur aberrante, a voir si filter avec dplyr est préférable niveau optimisation (je ne pense pas)
sum_mots<-sum(test$nb_mots)             #ON COMPTE LE NOMBRE DE MOTS
ggplot(test, aes(x=nb_mots))+ #on peut aussi utiliser test %>% et direct utiliser le nom des variables plutot que d'indiquer $test a chaque fois
  geom_density(fill="dodgerblue", alpha=0.5)+ #or geom_historgram
  #scale_x_log10()
  labs(title=paste0("Total number of words in the corpus of tweets ","(n=",nrow(test),") : " ,sum_mots), x="Number of words per tweet", y="Density")+
  geom_vline(xintercept=mean(test$nb_mots), size=0.7, color="red")
  #geom_vline(xintercept=median(test$nb_mots), size=0.7, color="orange")
  geom_text(aes(x=mean_salary+60000, label=paste0("Mean\n",mean(test$nb_mots)), y=1.9))

#source : https://datavizpyr.com/add-vertical-line-to-density-plot-with-ggplot2/

```

```{r tweet text cleaning remove mentions, urls, emojis, numbers, punctuations, etc}

Textprocessing <- function(tweet_text)
  {
  tweet_text <- gsub('\\p{So}|\\p{Cn}', '', tweet_text, perl = TRUE) #delete emotes
  tweet_text <- gsub('http\\S+\\s*', '', tweet_text) ## Remove URLs
  tweet_text <- gsub('\\b+RT', '', tweet_text) ## Remove RT
  tweet_text <- gsub('#', '', tweet_text) ## Remove Hashtags
  tweet_text <- gsub('@\\S+', '', tweet_text) ## Remove Mentions
  #tweet_text <- gsub('[[:punct:]]', '', tweet_text) ## Remove Punctuations
  tweet_text <- gsub("(?!')[[:punct:]]", "", tweet_text, perl = TRUE) #remove ponctuations except apostrophe
  tweet_text <- gsub('[[:digit:]]', '', tweet_text) #remove numeric values
  tweet_text <- str_remove_all(tweet_text,"»|«") #remove fancy quotation marks « and »
  tweet_text <- gsub("^[[:space:]]*","",tweet_text) ## Remove leading whitespaces
  tweet_text <- tolower(tweet_text)
  tweet_text <- gsub("[[:space:]]*$","",tweet_text) ## Remove trailing whitespaces
  tweet_text <- gsub(' +',' ',tweet_text) ## Remove extra whitespaces 
  
  
  #tweet_text <- gsub("http[[:alnum:]]*",'', tweet_text)
  #tweet_text <- gsub("[^\x01-\x7F]", "", tweet_text) ## Delete all non-ASCII characters
  #tweet_text <- gsub('[[:cntrl:]]', '', tweet_text) ## Remove Controls and special characters
  #tweet_text <- gsub("\\d", '', tweet_text) ## Remove Controls and special characters

  
  return (tweet_text)
}
```

```{r main}
test <- mutate(test, text_clean = "")
#test <- test %>% add_column(text_clean = "" )  #on crée une colonne "text_clean" initialisée à vide
nrow <- nrow(test) #nombre de lignes
ncol <- ncol(test) #nombre de colonnes
index_text_column <- which(colnames(test)=="text") #on récupère l'index de la colonne "text" 
for (i in 1:nrow) 
  {
    test[i,ncol] <- Textprocessing(test[i,index_text_column])
}

#test  <- mutate(test, created_at_day = format(test$created_at, tz = "EST", usetz = TRUE))


```


```{r tokenization}
test <- mutate(test, text_tokens = tokenizers::tokenize_words(text_clean))
```

```{r}
#sample <- head(data_covid, 10000)
#nrow <- nrow(sample)
#ncol <- ncol(sample)

sample <- filter(data_covid, mois == "01" %>% slice(1:100))
                 
for (i in 1:12) 
  { 
    ifelse (i>9, unite <- "" , unite <- "0")
    mois <- filter(data_covid, mois == paste0(unite,i)) %>% slice(1:100)
    sample <- rbindlist(lapply(sample,mois))
}

saveRDS(sample,"./sample.rds")
#sample$nb_mots <- str_count(sample$text, " ")+1
#sum_mots <- sum(sample$nb_mots)
#ggplot(sample, aes(x=nb_mots))+
#  geom_histogram(fill="deepskyblue3")+
#  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r lemme01, echo = FALSE}
cnlp_init_string(locale="fr_FR")
cnlp_init_udpipe(model_name="french")
cnlp_init_spacy(model_name="fr")
cnlp_init_corenlp(lang="fr")

token<-obj$token %>%
  group_by(doc_id) %>%
  summarize(length = n())

foo<-token %>%filter(length>1)

m_token=median(as.numeric(token$length))

g0<-ggplot(foo,aes(length))+
  geom_histogram(binwidth = 1,  fill="firebrick3")+geom_vline(xintercept = m_token, linetype="solid", color = "royalblue3", size=1.2)+
  theme_minimal()+
  labs(title = "Distribution des longueurs de texte ( tokens)",subtitle=paste0("mÃ©diane : ",m_token), caption = "data covid19",x="nombre de mots par tweet",y="Effectifs")+xlim(0,70)
g0
```

```{r}
sample <- head(data_covid, 1000000)
nrow <- nrow(sample)
ncol <- ncol(sample)
sample$nb_mots <- str_count(sample$text, " ")+1
sum_mots <- sum(sample$nb_mots)
ggplot(sample, aes(x=nb_mots))+
  geom_histogram(fill="deepskyblue3")+
  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r}
#récupère tous les tweets avec un hashtag #coronavirus
#utiliser regex pour récupérer mots : ce qui parle de la maladie
#corona - covid - SRAS - vaccin - geste(s) barrière(s) - nettoyage des mains - masque - gel - hydroalcoolique
#acteurs = macron, veran
#institutions = conseil scientifique

#créer des colonnes supplémentaires avec mutate(

test <- mutate(test, hashtags = gsub('#|"', '',str_extract_all(text, "#\\S+")))

```


```{r}
#trie tweets originaux par nombre de favoris
sample %>%
  filter(tweet_type == "original") %>%
  arrange(desc(favorite_count)) %>%
  view("tri_tweets_favori")

#corrélation temporelle entre une t-1 t t+1 t+2
#identifier les cascades => se lancer dans une simulation avec R
#on génère des tweets, on attribue à ces tweets une certaine probabilité d'et^re cité/tweeté
#ça crée de nouveaux éléments qui pourront eux-mêmes être retweetés
#et relier ça à des caractéristiques des users genre le nombre de followers => se focaliser sur les comptes eux-mêmes
## Evolution de mots cibles
#On utilise ici une idée simple mais pas facile à mettre en oeuvre. Même en lemmatisant de nombreux mots proches seront considéré comme distincts : mille-feuilles, millesfeuilles, millefeuille, milles-feuilles, . Pir encore : corona, coronavirus, coronavir, coronarvirus etc... La simplicité de la méthode consiste à définir des motifs. Ici la racine est *corona* le * représente n'importe qu'elle caractère, on aurait pu réduire à coron, mais on aurait fait une confusion avec coroner. Ce principe est systématiser dans la méthode des regex, ou expressions régulières, où un jeu de convention limitée, permet de détruire une grande variété de motifs morphologique : un numero de téléphone, une url, un prix, .
#La maitrise de ce langage, car s'en est un un , est difficile, et relève plus de l'art que de la science, un art de résolution de problème logique. Mais même avec des expressions simples, élémentaires, on peut réaliser des tâche intéressantes.
#la base contient 5,447 millions de tokens (mots) pour 273 389 textes. (ce n'est pas exact c'est le double il faut redonner les valeurs exactes)
```

