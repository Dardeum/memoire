---
title: "Memoire R"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(rtweet)
library(lubridate)
library(dplyr)
library(data.table)
```

```{r}
setwd("C:/Users/Olivier/Desktop/Cours IREN/Memoire/")
data_covid <- readRDS("df_0001002.rds")
```


```{r}
sample <- sample_n(data_covid, 20000)
```

```{r}
#sample <- head(data_covid, 10000)
#nrow <- nrow(sample)
#ncol <- ncol(sample)

sample <- filter(data_covid, mois == "01" %>% slice(1:100))
                 
for (i in 2:12) 
  { 
    ifelse (i>9, unite <- "" , unite <- "0")
    mois <- filter(data_covid, mois == paste0(unite,i)) %>% slice(1:100)
    sample <- rbindlist(lapply(sample,mois))
}

saveRDS(sample,"./sample.rds")
#sample$nb_mots <- str_count(sample$text, " ")+1
#sum_mots <- sum(sample$nb_mots)
#ggplot(sample, aes(x=nb_mots))+
#  geom_histogram(fill="deepskyblue3")+
#  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r}
sample <- head(data_covid, 1000000)
nrow <- nrow(sample)
ncol <- ncol(sample)
sample$nb_mots <- str_count(sample$text, " ")+1
sum_mots <- sum(sample$nb_mots)
ggplot(sample, aes(x=nb_mots))+
  geom_histogram(fill="deepskyblue3")+
  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r}
#récupère tous les tweets avec un hashtag #coronavirus
#utiliser regex pour récupérer mots : ce qui parle de la maladie
#corona - covid - SRAS - vaccin - geste(s) barrière(s) - nettoyage des mains - masque - gel - hydroalcoolique
#acteurs = macron, veran
#institutions = conseil scientifique
#créer des colonnes supplémentaires avec mutate()
sample %>%
  mutate(corona = str_detect(text, "#coronavirus"))
  #mutate(corona == ifelse(str_detect(text, "#coronavirus"),1,0))
  #filter(str_detect(text, "#coronavirus")) %>%
  view("coronavirus hastag")#%>%
#ggplot(sample, mapping = aes(x= mois, y = 
```


```{r}
#trie tweets originaux par nombre de favoris
sample %>%
  filter(tweet_type == "original") %>%
  arrange(desc(favorite_count)) %>%
  view("tri_tweets_favori")

#corrélation temporelle entre une t-1 t t+1 t+2
#identifier les cascades => se lancer dans une simulation avec R
#on génère des tweets, on attribue à ces tweets une certaine probabilité d'et^re cité/tweeté
#ça crée de nouveaux éléments qui pourront eux-mêmes être retweetés
#et relier ça à des caractéristiques des users genre le nombre de followers => se focaliser sur les comptes eux-mêmes
## Evolution de mots cibles
On utilise ici une idée simple mais pas facile à mettre en oeuvre. Même en lemmatisant de nombreux mots proches seront considéré comme distincts : mille-feuilles, millesfeuilles, millefeuille, milles-feuilles, . Pir encore : corona, coronavirus, coronavir, coronarvirus etc... La simplicité de la méthode consiste à définir des motifs. Ici la racine est *corona* le * représente n'importe qu'elle caractère, on aurait pu réduire à coron, mais on aurait fait une confusion avec coroner. Ce principe est systématiser dans la méthode des regex, ou expressions régulières, où un jeu de convention limitée, permet de détruire une grande variété de motifs morphologique : un numero de téléphone, une url, un prix, .
La maitrise de ce langage, car s'en est un un , est difficile, et relève plus de l'art que de la science, un art de résolution de problème logique. Mais même avec des expressions simples, élémentaires, on peut réaliser des tâche intéressantes.
la base contient 5,447 millions de tokens (mots) pour 273 389 textes. (ce n'est pas exact c'est le double il faut redonner les valeurs exactes)
```{r pos2, fig.width=9}
df$day<-as.numeric(format(df$parsed_created_at, "%d")) # jour
df$month<-as.numeric(format(df$parsed_created_at, "%m")) # mois
df$hour<-as.numeric(format(df$parsed_created_at, "%H")) # heure
df$year<-2020 # heure
df$date<-paste0("2020","-",df$month,"-",df$day)
df$date2 <- as.POSIXct(strptime(df$date, "%Y-%m-%d"))
df<-df %>% mutate(Jour=ifelse(month==1,day,ifelse(month==2,day+31,ifelse(month==3,day+59,ifelse(month==4, day+90,ifelse(month==5,120+day,151+day))))))
df$Jour<-as.factor(df$Jour)
token<- token%>%select(-doc_id)
foo<-cbind(df,token)
foo<-obj$token %>% left_join(foo)
foo$date<-paste0("2020","-",foo$month,"-",foo$day)
foo$date2 <- as.POSIXct(strptime(foo$date, "%Y-%m-%d"))
# tres important les regex pour retrouver les variantes
#dans la reg ^ veut dire qu'on cherche uniquement le mot qui commence par ^mot
#exemple : anticorona ne sera pas pris
df_lemma<-foo %>%
  mutate(lemma=ifelse(grepl("^corona.*",lemma),"corona",lemma),
         lemma=ifelse(grepl(".*covid.*",lemma),"covid",lemma),
         lemma=ifelse(grepl("^chloro.*",lemma),"chloroquine",lemma),
         lemma=ifelse(grepl("^masqu.*",lemma),"masque",lemma),
         lemma=ifelse(grepl("^confine.*",lemma),"confinement",lemma),
         lemma=ifelse(grepl("d[eéèêë]confin.*",lemma),"déconfinement",lemma),
         lemma=ifelse(grepl("^gouve.*",lemma),"gouvernement",lemma),
         lemma=ifelse(grepl("^politi.*",lemma),"politique",lemma),
         lemma=ifelse(grepl("sanit.*",lemma),"sanitaire",lemma),
         lemma=ifelse(grepl("^gel.*",lemma),"gel",lemma),
         lemma=ifelse(grepl("^geste.*",lemma),"geste",lemma),
         lemma=ifelse(grepl("^test.*",lemma),"test",lemma),
         lemma=ifelse(grepl("^t[e,é])l[e,é].*trav.*",lemma),"télétravail",lemma),
         lemma=ifelse(grepl("^h([o,ô]|[os,ôs])pital.*",lemma),"hôpital",lemma),
         upos=ifelse(grepl("^[eéèêë]tre",lemma),"VERB", upos),
         upos=ifelse(grepl("^avoir",lemma),"VERB",upos)
         )
df_lemma<-df_lemma %>%
  mutate(lemma=ifelse(grepl("^chin.*",lemma),"chine",lemma),
         lemma=ifelse(grepl("^fran[c,ç][e,a].*",lemma),"france",lemma),
         lemma=ifelse(grepl("^alle.*",lemma),"allemagne",lemma),
         lemma=ifelse(grepl("^itali*",lemma),"italie",lemma),
         lemma=ifelse(grepl("^[é,e]tat.*uni.*",lemma),"etats-unis",lemma),
         lemma=ifelse(grepl("^br[e,é]sil.*",lemma),"bresil",lemma),
         lemma=ifelse(grepl("^singapour*",lemma),"singapour",lemma),
         lemma=ifelse(grepl("^viet*",lemma),"vietnam",lemma),
         lemma=ifelse(grepl("^afri[c,q].*",lemma),"afrique",lemma),
         )
#juste pour avoir une idée de ce qu'on ramène dans le filet
#fooz<-df_lemma %>% filter(grepl("h([o,ô]|[os,ôs])pital.*",lemma))%>%group_by(lemma) %>% mutate(n=1)%>% summarise(n=sum(n))
#tokenise dans quelle phrase, quelle position
#je dois créer une colonne avec mutate
foo2<-df_lemma %>%
  group_by(date2,lemma) %>% mutate(n=1)%>%
  summarise(frequence=sum(n)) %>%
  ungroup()  %>%
  group_by(date2) %>%
  arrange(date2, desc(frequence), lemma)
token_j<-foo2 %>% group_by(date2) %>% summarise(total=sum(frequence))
#on annote avec package spacy de python
#corenlp annotateur de stanford
#udpipe ligne d'initi
#cnlp_init_spacy('fr_core_news_sm')spacy.load('fr_core_news_sm')
#cnlp_download_corenlp("fr")
#cnlp_init_corenlp("fr")
# avant c'est à tester commence l'annotation veritable
cnlp_init_udpipe(model_name = "french") #je veux le modèle qui correspond au FR
#obj <- cnlp_annotate(text) #
#saveRDS(obj,"objCovid.rds") = nombre de text * nb de mots par text
obj<-readRDS(file="objCovid.rds")
```
Examinons les tokens/mots obtenus de manière brut : il peuvent être de la ponctuation,  des acronymes, etc.
```{r lemme01, echo = FALSE}
#library(cleanNLP) 
token<-obj$token %>% #cherche la liste des tokens
  group_by(doc_id) %>% #groupage par doc_id
  summarize(length = n()) #résume en comptant le nb de (tokens)l par tweet
foo<-token %>%filter(length>1)

```
```

```{r}
#trie tweets originaux par nombre de favoris
sample %>%
  ggplot(aes(mois, nb_mots))+
    geom_point()+
    geom_line(colour = "red")
```
