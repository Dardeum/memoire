---
title: "Memoire R"
output: html_notebook
---

```{r}
library(tidyverse)
library(readr)
library(rtweet)
library(lubridate)
library(dplyr)
library(data.table)
library(cleanNLP)
library(foreach)
```

```{r}
#setwd("C:/Users/Olivier/Desktop/Cours_IREN/Memoire/")
#data_covid50 <- readRDS("sample_50k.rds")
#data_covid <- readRDS("df_0001002.rds")
```

```{r fichier de travail}
#takes of sample of subset of n records so it's easier to work with
#sample <- sample_n(data_covid, 50000)
#sample <- mutate(sample, )
#head1kk <-head(data_covid, 1000)
#samplen30k <- sample_n(data_covid, 30000)
#saveRDS(head1kk,"head1k.rds")
test <- readRDS("head1k.rds")
```

```{r}
Textprocessing <- function(tweet_text)
  {
  tweet_text <- gsub("http[[:alnum:]]*",'', tweet_text)
  tweet_text <- gsub("[^\x01-\x7F]", "", tweet_text) ## Delete all non-ASCII characters
  tweet_text <- gsub('http\\S+\\s*', '', tweet_text) ## Remove URLs
  tweet_text <- gsub('\\b+RT', '', tweet_text) ## Remove RT
  tweet_text <- gsub('#\\S+', '', tweet_text) ## Remove Hashtags
  tweet_text <- gsub('@\\S+', '', tweet_text) ## Remove Mentions
  tweet_text <- gsub('[[:cntrl:]]', '', tweet_text) ## Remove Controls and special characters
  tweet_text <- gsub("\\d", '', tweet_text) ## Remove Controls and special characters
  tweet_text <- gsub('[[:punct:]]', '', tweet_text) ## Remove Punctuations
  tweet_text <- gsub("^[[:space:]]*","",tweet_text) ## Remove leading whitespaces
  tweet_text <- gsub("[[:space:]]*$","",tweet_text) ## Remove trailing whitespaces
  tweet_text <- gsub(' +',' ',tweet_text) ## Remove extra whitespaces
  return (tweet_text)
}
```

```{r remove mentions, urls, emojis, numbers, punctuations, etc.}
#create new column text_noemote by removing emotes from tweets
#test <- test %>% add_column(text_clean = "" )  #on crée une colonne "text_clean" initialisée à vide
nrow <- nrow(test) #nombre de lignes
ncol <- ncol(test) #nombre de colonnes
index_text_column <- which(colnames(test)=="text") #on récupère l'index de la colonne "text" 
for (i in 1:nrow) 
  {
    test[i,ncol] <- Textprocessing(test[i,index_text_column])
}

```


```{r}
#sample <- head(data_covid, 10000)
#nrow <- nrow(sample)
#ncol <- ncol(sample)

sample <- filter(data_covid, mois == "01" %>% slice(1:100))
                 
for (i in 1:12) 
  { 
    ifelse (i>9, unite <- "" , unite <- "0")
    mois <- filter(data_covid, mois == paste0(unite,i)) %>% slice(1:100)
    sample <- rbindlist(lapply(sample,mois))
}

saveRDS(sample,"./sample.rds")
#sample$nb_mots <- str_count(sample$text, " ")+1
#sum_mots <- sum(sample$nb_mots)
#ggplot(sample, aes(x=nb_mots))+
#  geom_histogram(fill="deepskyblue3")+
#  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r}
sample <- head(data_covid, 1000000)
nrow <- nrow(sample)
ncol <- ncol(sample)
sample$nb_mots <- str_count(sample$text, " ")+1
sum_mots <- sum(sample$nb_mots)
ggplot(sample, aes(x=nb_mots))+
  geom_histogram(fill="deepskyblue3")+
  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")
```

```{r}
#récupère tous les tweets avec un hashtag #coronavirus
#utiliser regex pour récupérer mots : ce qui parle de la maladie
#corona - covid - SRAS - vaccin - geste(s) barrière(s) - nettoyage des mains - masque - gel - hydroalcoolique
#acteurs = macron, veran
#institutions = conseil scientifique
#créer des colonnes supplémentaires avec mutate()
sample %>%
  mutate(corona = str_detect(text, "#coronavirus"))
  #mutate(corona == ifelse(str_detect(text, "#coronavirus"),1,0))
  #filter(str_detect(text, "#coronavirus")) %>%
  view("coronavirus hastag")#%>%
#ggplot(sample, mapping = aes(x= mois, y = 
```


```{r}
#trie tweets originaux par nombre de favoris
sample %>%
  filter(tweet_type == "original") %>%
  arrange(desc(favorite_count)) %>%
  view("tri_tweets_favori")

#corrélation temporelle entre une t-1 t t+1 t+2
#identifier les cascades => se lancer dans une simulation avec R
#on génère des tweets, on attribue à ces tweets une certaine probabilité d'et^re cité/tweeté
#ça crée de nouveaux éléments qui pourront eux-mêmes être retweetés
#et relier ça à des caractéristiques des users genre le nombre de followers => se focaliser sur les comptes eux-mêmes
## Evolution de mots cibles
#On utilise ici une idée simple mais pas facile à mettre en oeuvre. Même en lemmatisant de nombreux mots proches seront considéré comme distincts : mille-feuilles, millesfeuilles, millefeuille, milles-feuilles, . Pir encore : corona, coronavirus, coronavir, coronarvirus etc... La simplicité de la méthode consiste à définir des motifs. Ici la racine est *corona* le * représente n'importe qu'elle caractère, on aurait pu réduire à coron, mais on aurait fait une confusion avec coroner. Ce principe est systématiser dans la méthode des regex, ou expressions régulières, où un jeu de convention limitée, permet de détruire une grande variété de motifs morphologique : un numero de téléphone, une url, un prix, .
#La maitrise de ce langage, car s'en est un un , est difficile, et relève plus de l'art que de la science, un art de résolution de problème logique. Mais même avec des expressions simples, élémentaires, on peut réaliser des tâche intéressantes.
#la base contient 5,447 millions de tokens (mots) pour 273 389 textes. (ce n'est pas exact c'est le double il faut redonner les valeurs exactes)
```

